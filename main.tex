\documentclass[acmsmall,nonacm,screen,review]{acmart}
\newif\ifEnableExtend
%\EnableExtendtrue
\EnableExtendfalse

\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{color}
\newcommand{\csch}[1]{{\color{red} Christian says: #1}}
\newcommand{\Is}       {:=}
\newcommand{\set}[1]{\left\{ #1\right\}}
\newcommand{\sodass}{\,:\,}
\newcommand{\setGilt}[2]{\left\{ #1\sodass #2\right\}}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xspace}
\usepackage{relsize}

\newtheorem{openproblem}{Open Problem}
\newcommand{\ie}{i.\,e.,\xspace}
\newcommand{\eg}{e.\,g.,\xspace}
\newcommand{\etal}{et~al.\xspace}
\newcommand{\cov}{\term{cov}\xspace}
\newcommand{\term}[1]{\textsl{#1}}
\newcommand{\Comment}[1]{\textsl{#1}}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\origVol}{origVol}
\DeclareMathOperator{\origDeg}{origDeg}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcopyright{none}
\copyrightyear{2021}
\acmYear{2021}
\acmDOI{}
\acmPrice{}
\acmISBN{}

\title{Beginner's Practical: \\ Hypergraph Conductance-based Clustering}
\author{Mariia Meshchaninova}
\email{mariia.meshchaninova@stud.uni-heidelberg.de, Informatik B.Sc. 100\%, 4740180}
\affiliation{%
  \institution{Heidelberg University}
  \streetaddress{Im Neuenheimer Feld 205}
  \city{Heidelberg}
  \state{Baden-WÃ¼rttemberg}
  \country{Germany}
  \postcode{69120}
}


\date{}

\begin{document}

\begin{abstract}
%Insert abstract here.
Hypergraph clustering is a problem of partitioning the nodes of 
a hypergraph into "true clusters" which are densely connected internally 
and sparsely connected externally. Conductance is a metric measuring the 
quality of a cut which is defined as the ratio of the weight of the cut 
to the minimal volume of one side of the cut.
In this project, we implement a hypergraph clustering algorithm that
heuristically tries to minimize the maximum value of the conductance 
over all cuts between clusters to find a high-quality hypergraph 
clustering. The algorithm is based on the existing shared-memory 
parallel algorithm for hypergraph partitioning \texttt{Mt-KaHyPar}.
\end{abstract}
\maketitle

\section{Introduction}
Hypergraph clustering is a problem of partitioning the nodes of a hypergraph
into "true clusters" which are densely connected internally and sparsely
connected externally. It is useful in many applications, such as 
semi-supervised learning \cite{ApplicationLearning}, 
gene expression analysis \cite{ApplicationGeneExpression}
and image segmentation \cite{ApplicationImageSegmentation}. 
Conductance metric, which is defined as the ratio of the weight of a given
cut to the minimal volume of one side of the cut, is widely used in graph 
clustering \cite{GraphConductance2023,GraphConductance2006} but to the best
of our knowledge, there are no partitioners directly optimizing
conductance-based metric of a hypergraph, while conductnace has been applied 
as a metric for evaluating the quality of a hypergraph clustering 
\cite{HyperSF}. Therefore in this project, we implement a heuristic hypergraph 
clustering algorithm with the objective of minimizing the maximum value of 
the conductance over all cuts between the clusters while partitioning 
the hypargraph in at most $k$ clusters. 

\noindent Our algorithm is based on the existing shared-memory parallel 
hypergraph $k$-way partitioner \texttt{Mt-KaHyPar} and uses the same 
multilevel approach as the original algorithm. First, the hypergraph is 
coarsened in stages by detecting and merging local node clusters, then the 
coarsest hypergraph is partitioned into $k$ clusters, and finally the 
initial partitioning is refined by uncoarsening the hypergraph nodes and 
applying a local search algorithm to improve the given objective. We 
implement two objective functions for the local search algorithm - 
\textit{conductance\_local} and \textit{conductance\_global} - 
and compare their performance on a benchmark set \texttt{ibm01...18} 
against a state of the art hypergraph partitioner \texttt{HyperModularity}.

\noindent In following, we define the used concepts in Section~\ref{sec:preliminaries}, 
then we describe related work on hypergraph partitioning and provide an overview
of the existing conductance-based clustering algorithms in Section~\ref{sec:related_work}. In 
Section~\ref{sec:implementation}, we describe the implementation of the new 
objectives for optimizing the conductance and their integration in \texttt{Mt-KaHyPar}. 
Afterwards, we present the experimental results of our implementation in Section~\ref{sec:experiments}. 
Finally, we conclude the paper and discuss future work in Section~\ref{sec:conclusion}.


%\noindent In this project, we implement a heuristic algorithm for hypergraph 
%lustering with the aim of partitioning the nodes of a hypergraph into 
%at most $k$ clusters, while minimising the maximum value of the conductance 
%over all the cuts between the clusters. The algorithm is based on the 
%existing shared-memory parallel hypergraph partitioner \texttt{Mt-KaHyPar}
%and uses the same multilevel approach as the original algorithm. First,
%the hypergraph is coarsened in stages by detecting and merging node
%commubities, then the coarsest hypergraph is partitioned into $k$ clusters, 
%and finally the initial partitioning is refined by uncoarsening the 
%hypergraph nodes and applying a local search algorithm to improve the given 
%objective. We implement two objective functions for the local search 
%algorithm - \textit{conductance\_local} and \textit{conductance\_global} - 
%ans compare their performance on a benchmark set of instances against



%performance on large benchmark sets of instances against other state-of-the-art 
%hypergraph partitioners \cite{MtKaHyPar2020}. The algorithm follows a multilevel
%approach by in stages coarsening a given hypergraph via detecting and merging 
%communities of the nodes, then partitioning the coarsest hypergraph into $k$ 
%clusters, and then iteratively refining the initial partitioning by uncoarsening
%the hypergraph nodes and applying a local search algorithm to improve the given 
%objective. The original algorithms supports several objectives, such as minimizing 
%the weight of the cut (\textit{cut} metric) or the sum of the external degrees of 
%the clusters (\textit{soed} metric). In this project, we implement two additional 
%objective functions for the \texttt{Mt-KaHyPar} algorithm to minimize the overall 
%maxmal conductance of a cut inbetween the clusters: \textit{conductance\_local} 
%and \textit{conductance\_global}.


\section{Preliminaries}
\label{sec:preliminaries}

In this section, we give an overview of the used concepts and notations and 
define conductance of a hypergraph.

\smallbreak
\noindent\textbf{Hypergraph} is a generalization of a graph, where a hyperedge connects 
one, two or more nodes. Formally, an edge-weighted hypergraph $H = (V, E, \omega)$ 
consists of a non-empty set of nodes $V$, a set of hyperedges (also called 
\textit{nets} or simply \textit{edges}) $E \subseteq 2^V$ and a weight function
for hyperedges $\omega: E \to \mathbb{N}$. In case of unweighted hyperedges, 
we set $\omega(e) = 1$ for all $e \in E$. 

\smallbreak
\noindent\textbf{$k$-way clustering} of a hypergraph $H = (V, E,\omega)$ is a
partitioning of the set of nodes $V$ into $k$ disjoint subsets 
$V_1, \dots, V_k$ called \textit{clusters}. Empty clusters are allowed, so
$k$ is only an upper bound on the final number of clusters. Our algorithm is 
allowed to eventially move all the nodes of a cluster to other clusters if it 
would be beneficial for the objective function. 

\smallbreak
\noindent\textbf{Cut} $\partial S$ of a hypergraph $H = (V, E, \omega)$ is partitioning
of the set of nodes $V$ into two disjoint non-empty subsets $S$ and 
$V \backslash S$. A hyperedge $e \in E$ is called a \textbf{cutting edge} of 
the cut $\partial S$ if it has at least one pin in $S$ and at least one pin in
$V \backslash S$. The weight of the cut is defined as the sum of the weights of
the cutting edges of the cut $\partial S$: 
\[\omega(\partial S) = \sum_{e \in \partial S} \omega(e)\]
Therefore in case of unweighted hyperedges, the weight of the cut is equal to 
the number of cutting edges of the cut.

\smallbreak
\noindent\textbf{Pin} of a hyperedge $e \in E$ is a node $v \in V$ such that $v \in e$.
A hyperedge with only one pin is called a \textbf{sinle-pin net}.
In the implementation, we use number of pins $PinNum_i(e)$ of a hyperedge 
$e \in E$ in a cluster $V_i$ to decide whether to move a node $v$ from one
cluster $V_i$ to another cluster $V_j$.

\smallbreak
\noindent\textbf{Weighted degree} of a node $v \in V$ is defined the same way in 
hypergraphs as in graphs. It is the sum of the weights of all hyperedges 
$e \in E$ that contain the node $v$: 
\[\deg_\omega(v) = \sum_{e\in E: v \in E} \omega(e)\]
As during the coarsening phase of the algorithm we merge nodes, we also use
\textbf{original weighted degree} of a node $v' \in V'$ of the coarsened hypergraph
$\origDeg_\omega(v')$ 
which is defined as the sum of the weighted degrees of all nodes of the
initial hypergraph $v \in V$ that were merged into $v'$ during the coarsening 
phase of the algorithm.

\smallbreak
\noindent\textbf{Volume} of a hypergraph $H = (V, E, c, \omega)$ is defined as the 
sum of the weighted degrees of all nodes $v \in V$. Analogously, we define the 
\textbf{volume of a cluster} $V_i$ as the sum of the weighted degrees of all 
nodes $v \in V_i$:
\[\vol(H) = \sum_{v \in V} \deg_\omega(v) \ \ \ \ \ \ \
\vol(V_i) = \sum_{v \in V_i} \deg_\omega(v)\]
And for the same reason as for the original weighted degree, we also define 
the \textbf{original volume} of a hypargraph and a cluster:
\[\origVol(H) = \sum_{v \in V} \origDeg_\omega(v) \ \ \ \ \ \ \
\origVol(V_i) = \sum_{v \in V_i} \origDeg_\omega(v)\]
This way, the original volume of a cluster or of the whole hypergraph 
in the coarsened hypergraph is equal to the corresponding volume in the initial 
- \textit{original} - hypergraph.

\smallbreak
\noindent\textbf{Conductance of a cut} $\partial S$ of a hypergraph 
$H = (V, E, \omega)$ is defined as the ratio of the weight of the cut to the 
minimum volume of one side of the cut. The maximal conductance of a cut In
a hypergraph is referred to as the \textbf{conductance of the hypergraph}:
\[{\varphi(S)} = \frac{\omega(\partial S)}{\min\{vol(S), vol(V \backslash S)\}}
\ \ \ \ \ \ 
{\varphi(V)} = \max_{\emptyset \subsetneq S \subsetneq V} \varphi(S)
\]
To use conductance as a quality metric for a hypergraph clustering with 
possibly more than two clusters, we define the 
\textbf{conductance of a hypergraph clustering} $V_1, \dots, V_k$ as the 
maximum conductance of all cuts between the clusters. Conveniently, this 
definition can be simplified to the maximal conductance of a cut $\partial V_i$ 
over all clusters $V_i$:
\[\varphi(V_1, \dots, V_k) 
:= \max_{\emptyset \neq I \subsetneq \{1, \dots, k\}} 
        \{\varphi(\cup_{i \in I} V_i)\} = \max_{i = 1 \dots k} \varphi(V_i)\]
A proof of this nice statement can be found in Appendix~\ref{appendix:a}.

\section{Related Work}
\label{sec:related_work}

There are several state of the art algorithms for graph clustering that 
aim to minimize the resulting conductance. Some of them are heuristic like 
the diffusion based \texttt{NIBBLE} \cite{Nibble} and \texttt{hk-relax} \cite{HKRelax}.
Other, such as degree ratio-based \texttt{PC\_de} \cite{PCde} have a theoretical
guaranatee of the resulting conductance. 

As for the hypergraph-clustering, to the best of our knowledge, there are no
existing algorithms that directly optimize a conductance-based objective. Instead,
state of the art hypergraph clustering algorithms, e.g. \texttt{HyperModularity} 
which we use for comparison with our algorithm, deliver high quality
hypergraph clustering by optimizing the modularity of the hypergraph
\cite{HyperModularity,ModularityComparison}.

The algorithm \texttt{Mt-KaHyPar} that we use as a base for our implementation also 
optimizes the modularity of the hypergraph in its coarsening stage. But in general,
it could be seen as a framework for hypergraph clustering with a user-implemented
objective, which is optimized during the initial partitioning of the coarsest hypergraph
and later in the uncoarsening stage consisting of label propagation and FR-refinement 
\cite{MtKaHyPar2020}.

\section{Implementation}
\label{sec:implementation}

\section{Experimental Results}
\label{sec:experiments}

\section{Conclusion}
\label{sec:conclusion}

\appendix
\section{Appendix}
\label{appendix:a}

\begin{theorem}
  Concuctance of a $k$-way partition $V_1, \dots, V_k$ is the maximal conductance 
  of a cut $V_i$ for $i = 1, \dots k$. 
\end{theorem}
\begin{proof}
Per definition of conductance of a $k$-way partition, there exists a subset 
$\emptyset \subsetneq I \subsetneq \{1, \dots, k\}$ such that 
$\varphi(V_1, \dots V_k) = \varphi(\cup_{i \in I} V_i)$. Without loss of
generality, we can assume that the volume of the union of the clusters
$V_i$ for $i \in I$ is less than or equal to the volume if the union of all the
other clusters $V_i$ for $i \notin I$. Then we can write:
\begin{align*}
\varphi(V_1, \dots, V_k) 
&= 
\frac{\omega(\partial (\cup_{i \in I} V_i))}
     {\min(\vol(\cup_{i \in I} V_i), \vol(\cup_{i \notin I} V_i))} 
=
\frac{\sum \{\omega(e) : e - \text{a cutting edge of } \partial(\cup_{i \in I} V_i)\}}
     {\vol(\cup_{i \in I} V_i)}
\\ & \leq
\frac{\sum_{i \in I} \omega(\partial V_i)}
     {\sum_{i \in I} \vol(V_i)} 
=
\frac{\sum_{i \in I} \vol(V_i) \cdot \frac{\omega(\partial V_i)}
                                          {\vol(V_i)}}
     {\sum_{i \in I} \vol(V_i)} 
\leq
\max_{i \in I}\frac{\omega(\partial V_i)}
                   {\vol(V_i)}
\\ &\leq
\max_{i \in I}\frac{\omega(\partial V_i)}
                   {\min(\vol(V_i), \vol(V \backslash V_i))}
=
\max_{i \in I} \varphi(V_i)
\\ &\leq 
\varphi(V_1, \dots, V_k)
\end{align*}
Thus, all inequalities are equalities, which means that the conductance of the
$k$-way partition is equal to the maximal conductance of a cut $V_i$ for
$i = 1, \dots k$.

It is interesting to note that with this we have also proved that the 
conductance of a hypergraph partition is equal to the maximum ratio of the 
weight of a cut $V_i$ to the volume of the cluster $V_i$ over all the clusters 
$V_1, \dots V_k$. This property, however, was not used in the implementation 
due to late discovery.
\end{proof}


\section{Gain for local search: naive?}

Information needed:
\begin{itemize}
    \item 
    the sum of all weights of the nets: $vol(V) = const$;
    \item
\underline{for each node}: 
    \begin{itemize}
        \item incident nets;
        \item weighted degree;
    \end{itemize}
    \item 
\underline{for each net $e$}: 
    \begin{itemize}
        \item 
number of pins in $e$: $|e|$;
        \item 
number of pins of $e$ which are currently in cluster $V_i$ for each cluster index $i$: $e.\#pins(V_i)$;
    \end{itemize}
    \item 
\underline{for each cluster index $i$}: 
    \begin{itemize}
        \item 
current volume: $vol(V_i)$;
        \item 
weights sum of cutting edges of the current $V_i$: $V_i.out\_weight$.
    \end{itemize}
\end{itemize}

Time needed: $\mathcal{O}(k \cdot deg(v) + k \cdot \log k)$...

\medskip

Calculating the gain for moving node $v$ from $V_i$ to $V_j$:
\begin{enumerate}
    \item 
(temporary) adjust volumes of $V_i$ and $V_j$: $\Theta(1)$
\[vol(V_i)' \xleftarrow{} vol(V_i) - \deg_\omega(v)\]
\[vol(V_j)' \xleftarrow{} vol(V_j) + \deg_\omega(v)\]
    \item 
(temporary) adjust $Vi.out\_weight$ and analogously for $Vj.out\_weight$ (but with reversed signs of $\omega(e)$ and $0, |e| - 1$ in conditions in ifs): $\Theta(\deg(v))$

For each incident to $v$ net $e \in E$:
\begin{itemize}
    \item[]
if $1 = e.\#pins(V_i) < |e|$: 
    \begin{itemize}
        \item[]
$e$ is a cutting net for the cut $V_i$, $v$ - its last pin in $V_i$

$\implies$ $e$ won't be a cutting edge after removal of $v$:
\[Vi.out\_weight' \xleftarrow{} V_i.out\_weight - \omega(e)\]
    \end{itemize}
    \item[]
else if $1 < e.\#pins(V_i) = |e|$:
\begin{itemize}
\item[]
$e$ was not a cutting net for the cut $V_i$, but it will be after the removal of $v$:
\[Vi.out\_weight' \xleftarrow{} V_i.out\_weight + \omega(e)\]
    \end{itemize}
\end{itemize}

\item
calculate new conductances of cuts $V_i$ and (analogously) $V_j$: $\Theta(1)$
\[\varphi(V_i) = \frac{V_i.out\_weight'}{\min\{vol(V_i)', vol(V) - vol(V_i)'\}}\]
\end{enumerate}

\medskip
\textbf{The question is, what should we take for gain from moving $v$}:
\begin{enumerate}
    \item 
decrease in maximal conductance between the cuts $V_i$ and $V_j$: \textit{easy};
    \item 
decrease in the overall maximal conductance: a PQ with $k$ current conductances $\varphi(V_1), \dots, \varphi(V_k)$ - additional $\mathcal{O}(k \log k)$ for finding the best $V_j$.
    \item[$\implies$] maybe the best according to criterion 1 from the best according to criterion 2... (\textbf{\textit{I believe, I should try all these variants and possibly some other...}})
\end{enumerate}
\hrule
%\end{idea}

\bigskip
A PQ with $k$ current conductances $\varphi(V_1), \dots, \varphi(V_k)$ is needed (?) to calculate the contribution of a net to the overall conductance of the partition (?):

\section{Contribution of a net to the overall conductance}
Objective function: conductance of the partition (also naive?):

\begin{itemize}
    \item each edge $e$ from the most expensive conduction-wise cut $V_i$ contributes by 
    \[\frac{\omega(e)}{\max\{vol(V_i), vol(V) - vol(V_i)\}}\]
    \item all other edges contribute by 0 (to ensure that the sum of all contributions is the current conductance...)
\end{itemize}
\hrule





\bibliographystyle{plainnat}
\bibliography{references.bib}

\end{document}
